{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0dc5e6de-fd8f-4589-a5bc-8169cdc12ca1",
   "metadata": {},
   "source": [
    "# Neural SDE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63d198be-5b0f-4d5d-a8d1-a63a3b24ed9d",
   "metadata": {},
   "source": [
    "This example constructs a neural SDE as a generative time series model.\n",
    "\n",
    "An SDE is, of course, random: it defines some distribution. Each sample is a whole path. Thus in modern machine learning parlance, an SDE is a generative time series model. This means it can be trained as a GAN, for example. This does mean we need a discriminator that consumes a path as an input; we use a CDE.\n",
    "\n",
    "Training an SDE as a GAN is precisely what this example does. Doing so will reproduce the following toy example, which is trained on irregularly-sampled time series:\n",
    "\n",
    "![ou](../imgs/neural_sde.png)\n",
    "\n",
    "**References:**\n",
    "\n",
    "Training SDEs as GANs:\n",
    "```bibtex\n",
    "@inproceedings{kidger2021sde1,\n",
    "    title={{N}eural {SDE}s as {I}nfinite-{D}imensional {GAN}s},\n",
    "    author={Kidger, Patrick and Foster, James and Li, Xuechen and Lyons, Terry J},\n",
    "    booktitle = {Proceedings of the 38th International Conference on Machine Learning},\n",
    "    pages = {5453--5463},\n",
    "    year = {2021},\n",
    "    volume = {139},\n",
    "    series = {Proceedings of Machine Learning Research},\n",
    "    publisher = {PMLR},\n",
    "}\n",
    "```\n",
    "\n",
    "Improved training techniques:\n",
    "```bibtex\n",
    "@incollection{kidger2021sde2,\n",
    "    title={{E}fficient and {A}ccurate {G}radients for {N}eural {SDE}s},\n",
    "    author={Kidger, Patrick and Foster, James and Li, Xuechen and Lyons, Terry},\n",
    "    booktitle = {Advances in Neural Information Processing Systems 34},\n",
    "    year = {2021},\n",
    "    publisher = {Curran Associates, Inc.},\n",
    "}\n",
    "```\n",
    "\n",
    "This example is available as a Jupyter notebook [here](https://github.com/patrick-kidger/diffrax/blob/main/examples/neural_sde.ipynb).\n",
    "\n",
    "!!! warning\n",
    "\n",
    "    This example will need a GPU to run efficiently.\n",
    "\n",
    "!!! danger \"Advanced example\"\n",
    "\n",
    "    This is an advanced example, due to the complexity of the modelling techniques used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "350ecd31-c6f3-4cff-adbc-2f880c40f11a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union\n",
    "\n",
    "import diffrax\n",
    "import equinox as eqx  # https://github.com/patrick-kidger/equinox\n",
    "import jax\n",
    "import jax.nn as jnn\n",
    "import jax.numpy as jnp\n",
    "import jax.random as jrandom\n",
    "import matplotlib.pyplot as plt\n",
    "import optax  # https://github.com/deepmind/optax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88968c8a-7c63-4557-a332-75565b7c8721",
   "metadata": {},
   "source": [
    "LipSwish activation functions are a good choice for the discriminator of an SDE-GAN. (Their use here was introduced in the second reference above.)\n",
    "For simplicity we will actually use LipSwish activations everywhere, even in the generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "df41f97b-8b00-49c4-84fe-b35f340b7be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lipswish(x):\n",
    "    return 0.909 * jnn.silu(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9415f14e-9195-4179-9430-0983faecab39",
   "metadata": {},
   "source": [
    "Now set up the vector fields appearing on the right hand side of each differential equation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "592dad43-7a89-4485-8b74-7855931d2526",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VectorField(eqx.Module):\n",
    "    scale: Union[int, jnp.ndarray]\n",
    "    mlp: eqx.nn.MLP\n",
    "\n",
    "    def __init__(self, hidden_size, width_size, depth, scale, *, key, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        scale_key, mlp_key = jrandom.split(key)\n",
    "        if scale:\n",
    "            self.scale = jrandom.uniform(\n",
    "                scale_key, (hidden_size,), minval=0.9, maxval=1.1\n",
    "            )\n",
    "        else:\n",
    "            self.scale = 1\n",
    "        self.mlp = eqx.nn.MLP(\n",
    "            in_size=hidden_size + 1,\n",
    "            out_size=hidden_size,\n",
    "            width_size=width_size,\n",
    "            depth=depth,\n",
    "            activation=lipswish,\n",
    "            final_activation=jnn.tanh,\n",
    "            key=mlp_key,\n",
    "        )\n",
    "\n",
    "    def __call__(self, t, y, args):\n",
    "        return self.scale * self.mlp(jnp.concatenate([t[None], y]))\n",
    "\n",
    "\n",
    "class ControlledVectorField(eqx.Module):\n",
    "    scale: Union[int, jnp.ndarray]\n",
    "    mlp: eqx.nn.MLP\n",
    "    control_size: int\n",
    "    hidden_size: int\n",
    "\n",
    "    def __init__(\n",
    "        self, control_size, hidden_size, width_size, depth, scale, *, key, **kwargs\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "        scale_key, mlp_key = jrandom.split(key)\n",
    "        if scale:\n",
    "            self.scale = jrandom.uniform(\n",
    "                scale_key, (hidden_size, control_size), minval=0.9, maxval=1.1\n",
    "            )\n",
    "        else:\n",
    "            self.scale = 1\n",
    "        self.mlp = eqx.nn.MLP(\n",
    "            in_size=hidden_size + 1,\n",
    "            out_size=hidden_size * control_size,\n",
    "            width_size=width_size,\n",
    "            depth=depth,\n",
    "            activation=lipswish,\n",
    "            final_activation=jnn.tanh,\n",
    "            key=mlp_key,\n",
    "        )\n",
    "        self.control_size = control_size\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "    def __call__(self, t, y, args):\n",
    "        return self.scale * self.mlp(jnp.concatenate([t[None], y])).reshape(\n",
    "            self.hidden_size, self.control_size\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4633fb9-97af-4e22-b903-40a1fc6c92ad",
   "metadata": {},
   "source": [
    "Now set up the neural SDE (the generator) and the neural CDE (the discriminator).\n",
    "\n",
    "- Note the use of very large step sizes. By using a large step size we essentially \"bake in\" the discretisation. This is quite a standard thing to do to decrease computational costs, when the vector field is a pure neural network. (You can reduce the step size here if you want to -- which will increase the computational cost, of course.)\n",
    "\n",
    "- Note the `clip_weights` method on the CDE -- this is part of imposing the Lipschitz condition on the discriminator of a Wasserstein GAN.\n",
    "(The other thing doing this is the use of those LipSwish activation functions we saw earlier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a4c157fe-4c86-4e15-9020-b523b517ebce",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralSDE(eqx.Module):\n",
    "    initial: eqx.nn.MLP\n",
    "    vf: VectorField  # drift\n",
    "    cvf: ControlledVectorField  # diffusion\n",
    "    readout: eqx.nn.Linear\n",
    "    initial_noise_size: int\n",
    "    noise_size: int\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        data_size,\n",
    "        initial_noise_size,\n",
    "        noise_size,\n",
    "        hidden_size,\n",
    "        width_size,\n",
    "        depth,\n",
    "        *,\n",
    "        key,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "        initial_key, vf_key, cvf_key, readout_key = jrandom.split(key, 4)\n",
    "\n",
    "        self.initial = eqx.nn.MLP(\n",
    "            initial_noise_size, hidden_size, width_size, depth, key=initial_key\n",
    "        )\n",
    "        self.vf = VectorField(hidden_size, width_size, depth, scale=True, key=vf_key)\n",
    "        self.cvf = ControlledVectorField(\n",
    "            noise_size, hidden_size, width_size, depth, scale=True, key=cvf_key\n",
    "        )\n",
    "        self.readout = eqx.nn.Linear(hidden_size, data_size, key=readout_key)\n",
    "\n",
    "        self.initial_noise_size = initial_noise_size\n",
    "        self.noise_size = noise_size\n",
    "\n",
    "    def __call__(self, ts, *, key):\n",
    "        t0 = ts[0]\n",
    "        t1 = ts[-1]\n",
    "        # Very large dt0 for computational speed\n",
    "        dt0 = jnp.array(1.0)\n",
    "        init_key, bm_key = jrandom.split(key, 2)\n",
    "        init = jrandom.normal(init_key, (self.initial_noise_size,))\n",
    "        control = diffrax.VirtualBrownianTree(\n",
    "            t0=t0, t1=t1, tol=dt0 / 2, shape=(self.noise_size,), key=bm_key\n",
    "        )\n",
    "        vf = diffrax.ODETerm(self.vf)  # Drift term\n",
    "        cvf = diffrax.ControlTerm(self.cvf, control)  # Diffusion term\n",
    "        terms = diffrax.MultiTerm(vf, cvf)\n",
    "        # ReversibleHeun is a cheap choice of SDE solver. We could also use Euler etc.\n",
    "        solver = diffrax.ReversibleHeun()\n",
    "        y0 = self.initial(init)\n",
    "        saveat = diffrax.SaveAt(ts=ts)\n",
    "        sol = diffrax.diffeqsolve(terms, solver, t0, t1, dt0, y0, saveat=saveat)\n",
    "        return jax.vmap(self.readout)(sol.ys)\n",
    "\n",
    "\n",
    "class NeuralCDE(eqx.Module):\n",
    "    initial: eqx.nn.MLP\n",
    "    vf: VectorField\n",
    "    cvf: ControlledVectorField\n",
    "    readout: eqx.nn.Linear\n",
    "\n",
    "    def __init__(self, data_size, hidden_size, width_size, depth, *, key, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        initial_key, vf_key, cvf_key, readout_key = jrandom.split(key, 4)\n",
    "\n",
    "        self.initial = eqx.nn.MLP(\n",
    "            data_size + 1, hidden_size, width_size, depth, key=initial_key\n",
    "        )\n",
    "        self.vf = VectorField(hidden_size, width_size, depth, scale=False, key=vf_key)\n",
    "        self.cvf = ControlledVectorField(\n",
    "            data_size, hidden_size, width_size, depth, scale=False, key=cvf_key\n",
    "        )\n",
    "        self.readout = eqx.nn.Linear(hidden_size, 1, key=readout_key)\n",
    "\n",
    "    def __call__(self, ts, ys):\n",
    "        # Interpolate data into a continuous path.\n",
    "        ys = diffrax.linear_interpolation(\n",
    "            ts, ys, replace_nans_at_start=0.0, fill_forward_nans_at_end=True\n",
    "        )\n",
    "        init = jnp.concatenate([ts[0, None], ys[0]])\n",
    "        control = diffrax.LinearInterpolation(ts, ys)\n",
    "        vf = diffrax.ODETerm(self.vf)\n",
    "        cvf = diffrax.ControlTerm(self.cvf, control)\n",
    "        terms = diffrax.MultiTerm(vf, cvf)\n",
    "        solver = diffrax.ReversibleHeun()\n",
    "        t0 = ts[0]\n",
    "        t1 = ts[-1]\n",
    "        dt0 = jnp.array(1.0)\n",
    "        y0 = self.initial(init)\n",
    "        # Have the discriminator produce an output at both `t0` *and* `t1`.\n",
    "        # The output at `t0` has only seen the initial point of a sample. This gives\n",
    "        # additional supervision to the distribution learnt for the initial condition.\n",
    "        # The output at `t1` has seen the entire path of a sample. This is needed to\n",
    "        # actually learn the evolving trajectory.\n",
    "        saveat = diffrax.SaveAt(t0=True, t1=True)\n",
    "        sol = diffrax.diffeqsolve(terms, solver, t0, t1, dt0, y0, saveat=saveat)\n",
    "        return jax.vmap(self.readout)(sol.ys)\n",
    "\n",
    "    @eqx.filter_jit\n",
    "    def clip_weights(self):\n",
    "        leaves, treedef = jax.tree_util.tree_flatten(\n",
    "            self, is_leaf=lambda x: isinstance(x, eqx.nn.Linear)\n",
    "        )\n",
    "        new_leaves = []\n",
    "        for leaf in leaves:\n",
    "            if isinstance(leaf, eqx.nn.Linear):\n",
    "                lim = 1 / leaf.out_features\n",
    "                leaf = eqx.tree_at(\n",
    "                    lambda x: x.weight, leaf, leaf.weight.clip(-lim, lim)\n",
    "                )\n",
    "            new_leaves.append(leaf)\n",
    "        return jax.tree_util.tree_unflatten(treedef, new_leaves)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb8bc8f0-2309-43ea-a23f-7e7f558e9691",
   "metadata": {},
   "source": [
    "Next, the dataset. This follows the trajectories you can see in the picture above. (Namely positive drift with mean-reversion and time-dependent diffusion.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a181d457-2ff5-4eac-8943-ca9e83faeb26",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "@jax.vmap\n",
    "def get_data(key):\n",
    "    bm_key, y0_key, drop_key = jrandom.split(key, 3)\n",
    "\n",
    "    mu = 0.02\n",
    "    theta = 0.1\n",
    "    sigma = 0.4\n",
    "\n",
    "    t0 = 0\n",
    "    t1 = 63\n",
    "    t_size = 64\n",
    "\n",
    "    def drift(t, y, args):\n",
    "        return mu * t - theta * y\n",
    "\n",
    "    def diffusion(t, y, args):\n",
    "        return 2 * sigma * t / t1\n",
    "\n",
    "    bm = diffrax.UnsafeBrownianPath(shape=(), key=bm_key)\n",
    "    drift = diffrax.ODETerm(drift)\n",
    "    diffusion = diffrax.ControlTerm(diffusion, bm)\n",
    "    terms = diffrax.MultiTerm(drift, diffusion)\n",
    "    solver = diffrax.Euler()\n",
    "    dt0 = 0.1\n",
    "    y0 = jrandom.uniform(y0_key, (1,), minval=-1, maxval=1)\n",
    "    ts = jnp.linspace(t0, t1, t_size)\n",
    "    saveat = diffrax.SaveAt(ts=ts)\n",
    "    sol = diffrax.diffeqsolve(\n",
    "        terms, solver, t0, t1, dt0, y0, saveat=saveat, adjoint=diffrax.DirectAdjoint()\n",
    "    )\n",
    "\n",
    "    # Make the data irregularly sampled\n",
    "    to_drop = jrandom.bernoulli(drop_key, 0.3, (t_size, 1))\n",
    "    ys = jnp.where(to_drop, jnp.nan, sol.ys)\n",
    "\n",
    "    return ts, ys\n",
    "\n",
    "\n",
    "def dataloader(arrays, batch_size, loop, *, key):\n",
    "    dataset_size = arrays[0].shape[0]\n",
    "    assert all(array.shape[0] == dataset_size for array in arrays)\n",
    "    indices = jnp.arange(dataset_size)\n",
    "    while True:\n",
    "        perm = jrandom.permutation(key, indices)\n",
    "        key = jrandom.split(key, 1)[0]\n",
    "        start = 0\n",
    "        end = batch_size\n",
    "        while end < dataset_size:\n",
    "            batch_perm = perm[start:end]\n",
    "            yield tuple(array[batch_perm] for array in arrays)\n",
    "            start = end\n",
    "            end = start + batch_size\n",
    "        if not loop:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1d48135-e8ff-411c-aa1c-2c3b6d55b4c4",
   "metadata": {},
   "source": [
    "Now the usual training step for GAN training.\n",
    "\n",
    "There is one neural-SDE-specific trick here: we increase the update size (i.e. the learning rate) for those parameters describing (and discriminating) the initial condition of the SDE. Otherwise the model tends to focus just on fitting just the rest of the data (i.e. the random evolution over time)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f7ec8e37-1aaa-4623-9601-9b21175708eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "@eqx.filter_jit\n",
    "def loss(generator, discriminator, ts_i, ys_i, key, step=0):\n",
    "    batch_size, _ = ts_i.shape\n",
    "    key = jrandom.fold_in(key, step)\n",
    "    key = jrandom.split(key, batch_size)\n",
    "    fake_ys_i = jax.vmap(generator)(ts_i, key=key)\n",
    "    real_score = jax.vmap(discriminator)(ts_i, ys_i)\n",
    "    fake_score = jax.vmap(discriminator)(ts_i, fake_ys_i)\n",
    "    return jnp.mean(real_score - fake_score)\n",
    "\n",
    "\n",
    "@eqx.filter_grad\n",
    "def grad_loss(g_d, ts_i, ys_i, key, step):\n",
    "    generator, discriminator = g_d\n",
    "    return loss(generator, discriminator, ts_i, ys_i, key, step)\n",
    "\n",
    "\n",
    "def increase_update_initial(updates):\n",
    "    get_initial_leaves = lambda u: jax.tree_util.tree_leaves(u.initial)\n",
    "    return eqx.tree_at(get_initial_leaves, updates, replace_fn=lambda x: x * 10)\n",
    "\n",
    "\n",
    "@eqx.filter_jit\n",
    "def make_step(\n",
    "    generator,\n",
    "    discriminator,\n",
    "    g_opt_state,\n",
    "    d_opt_state,\n",
    "    g_optim,\n",
    "    d_optim,\n",
    "    ts_i,\n",
    "    ys_i,\n",
    "    key,\n",
    "    step,\n",
    "):\n",
    "    g_grad, d_grad = grad_loss((generator, discriminator), ts_i, ys_i, key, step)\n",
    "    g_updates, g_opt_state = g_optim.update(g_grad, g_opt_state)\n",
    "    d_updates, d_opt_state = d_optim.update(d_grad, d_opt_state)\n",
    "    g_updates = increase_update_initial(g_updates)\n",
    "    d_updates = increase_update_initial(d_updates)\n",
    "    generator = eqx.apply_updates(generator, g_updates)\n",
    "    discriminator = eqx.apply_updates(discriminator, d_updates)\n",
    "    discriminator = discriminator.clip_weights()\n",
    "    return generator, discriminator, g_opt_state, d_opt_state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0421f671-dcc6-4338-a8f7-9fd17d6aee37",
   "metadata": {},
   "source": [
    "This is our main entry point. Try running `main()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b0581722-97fb-4771-94da-c65f9929e0f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(\n",
    "    initial_noise_size=5,\n",
    "    noise_size=3,\n",
    "    hidden_size=16,\n",
    "    width_size=16,\n",
    "    depth=1,\n",
    "    generator_lr=2e-5,\n",
    "    discriminator_lr=1e-4,\n",
    "    batch_size=1024,\n",
    "    steps=10000,\n",
    "    steps_per_print=200,\n",
    "    dataset_size=8192,\n",
    "    seed=5678,\n",
    "):\n",
    "    key = jrandom.PRNGKey(seed)\n",
    "    (\n",
    "        data_key,\n",
    "        generator_key,\n",
    "        discriminator_key,\n",
    "        dataloader_key,\n",
    "        train_key,\n",
    "        evaluate_key,\n",
    "        sample_key,\n",
    "    ) = jrandom.split(key, 7)\n",
    "    data_key = jrandom.split(data_key, dataset_size)\n",
    "\n",
    "    ts, ys = get_data(data_key)\n",
    "    _, _, data_size = ys.shape\n",
    "\n",
    "    generator = NeuralSDE(\n",
    "        data_size,\n",
    "        initial_noise_size,\n",
    "        noise_size,\n",
    "        hidden_size,\n",
    "        width_size,\n",
    "        depth,\n",
    "        key=generator_key,\n",
    "    )\n",
    "    discriminator = NeuralCDE(\n",
    "        data_size, hidden_size, width_size, depth, key=discriminator_key\n",
    "    )\n",
    "\n",
    "    g_optim = optax.rmsprop(generator_lr)\n",
    "    d_optim = optax.rmsprop(-discriminator_lr)\n",
    "    g_opt_state = g_optim.init(eqx.filter(generator, eqx.is_inexact_array))\n",
    "    d_opt_state = d_optim.init(eqx.filter(discriminator, eqx.is_inexact_array))\n",
    "\n",
    "    infinite_dataloader = dataloader(\n",
    "        (ts, ys), batch_size, loop=True, key=dataloader_key\n",
    "    )\n",
    "\n",
    "    for step, (ts_i, ys_i) in zip(range(steps), infinite_dataloader):\n",
    "        step = jnp.asarray(step)\n",
    "        generator, discriminator, g_opt_state, d_opt_state = make_step(\n",
    "            generator,\n",
    "            discriminator,\n",
    "            g_opt_state,\n",
    "            d_opt_state,\n",
    "            g_optim,\n",
    "            d_optim,\n",
    "            ts_i,\n",
    "            ys_i,\n",
    "            key,\n",
    "            step,\n",
    "        )\n",
    "        if (step % steps_per_print) == 0 or step == steps - 1:\n",
    "            total_score = 0\n",
    "            num_batches = 0\n",
    "            for ts_i, ys_i in dataloader(\n",
    "                (ts, ys), batch_size, loop=False, key=evaluate_key\n",
    "            ):\n",
    "                score = loss(generator, discriminator, ts_i, ys_i, sample_key)\n",
    "                total_score += score.item()\n",
    "                num_batches += 1\n",
    "            print(f\"Step: {step}, Loss: {total_score / num_batches}\")\n",
    "\n",
    "    # Plot samples\n",
    "    fig, ax = plt.subplots()\n",
    "    num_samples = min(50, dataset_size)\n",
    "    ts_to_plot = ts[:num_samples]\n",
    "    ys_to_plot = ys[:num_samples]\n",
    "\n",
    "    def _interp(ti, yi):\n",
    "        return diffrax.linear_interpolation(\n",
    "            ti, yi, replace_nans_at_start=0.0, fill_forward_nans_at_end=True\n",
    "        )\n",
    "\n",
    "    ys_to_plot = jax.vmap(_interp)(ts_to_plot, ys_to_plot)[..., 0]\n",
    "    ys_sampled = jax.vmap(generator)(\n",
    "        ts_to_plot, key=jrandom.split(sample_key, num_samples)\n",
    "    )[..., 0]\n",
    "    kwargs = dict(label=\"Real\")\n",
    "    for ti, yi in zip(ts_to_plot, ys_to_plot):\n",
    "        ax.plot(ti, yi, c=\"dodgerblue\", linewidth=0.5, alpha=0.7, **kwargs)\n",
    "        kwargs = {}\n",
    "    kwargs = dict(label=\"Generated\")\n",
    "    for ti, yi in zip(ts_to_plot, ys_sampled):\n",
    "        ax.plot(ti, yi, c=\"crimson\", linewidth=0.5, alpha=0.7, **kwargs)\n",
    "        kwargs = {}\n",
    "    ax.set_title(f\"{num_samples} samples from both real and generated distributions.\")\n",
    "    fig.legend()\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(\"neural_sde.png\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f182fe77-e4d2-4094-88c5-926cf2b1f8dd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diffrax-XVCcvQZQ",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
